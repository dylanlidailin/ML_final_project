{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22a126c",
   "metadata": {},
   "source": [
    "# Models optimization and training Notebook (s) (60 points)\n",
    "Must include complete training and optimization of:\n",
    "- A Penalized (Ridge, Lasso or ElasticNet) linear model (Linear Regression or Logistic Regression).\n",
    "- Support Vector Machine\n",
    "- Ensemble model (e.g. Random Forest or Gradient Boosting)\n",
    "- Neural network implemented in PyTorch\n",
    "\n",
    "You may use one combined notebook or separate notebooks for each model.\n",
    "\n",
    "REMEMBER: For the optimization and training stage, you must not use the test set you put aside in the prerequisite Final Project assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "daa16bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../Data/survey-lung-cancer.csv')\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=114514)\n",
    "train_df.to_csv('../Data/train_lung_cancer.csv', index=False)\n",
    "test_df.to_csv('../Data/test_lung_cancer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45cd865",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b1f53",
   "metadata": {},
   "source": [
    "StandardScaler of X_train, X_test + LabelEncoder of y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e400695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('../Data/train_lung_cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e09ea3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/cgypymwn2tj96mmjdmrs_2xr0000gn/T/ipykernel_4708/2699755995.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df['GENDER']= train_df['GENDER'].replace({'M':0, 'F':1})\n",
      "/var/folders/d9/cgypymwn2tj96mmjdmrs_2xr0000gn/T/ipykernel_4708/2699755995.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df['LUNG_CANCER']= train_df['LUNG_CANCER'].replace({'NO':0, 'YES':1})\n"
     ]
    }
   ],
   "source": [
    "features_df = train_df.drop(columns=['LUNG_CANCER'])\n",
    "features = features_df.columns.tolist()\n",
    "#print(features)\n",
    "# Standardization: turn 1 into 0 and 2 into 1\n",
    "train_df[features] = train_df[features].replace({1:0, 2:1})\n",
    "train_df['GENDER']= train_df['GENDER'].replace({'M':0, 'F':1})\n",
    "train_df['LUNG_CANCER']= train_df['LUNG_CANCER'].replace({'NO':0, 'YES':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef5ef8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X_train and y_train\n",
    "X_train = train_df.drop('LUNG_CANCER', axis=1)\n",
    "y_train = train_df['LUNG_CANCER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "deb1dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scaling the features\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf3cdf",
   "metadata": {},
   "source": [
    "## Training and optimization of the 4 models mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edc796",
   "metadata": {},
   "source": [
    "### A penalized linear model (Ridge, Lasso or ElasticNet) (Linear Regression or Logistic Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba95120",
   "metadata": {},
   "source": [
    "Lasso with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67acb0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso C value: {'C': 1}\n",
      "Best Lasso Score: 0.8622857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# We use 'liblinear' solver because it handles L1 penalty (Lasso) well\n",
    "# class_weight='balanced' helps with your class imbalance problem\n",
    "lasso_model = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42)\n",
    "\n",
    "# Grid Search to find the best regularization strength (C)\n",
    "# C is the inverse of regularization strength; smaller C = stronger regularization\n",
    "param_grid_lasso = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search_lasso = GridSearchCV(lasso_model, param_grid_lasso, cv=5, scoring='accuracy')\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Lasso C value:\", grid_search_lasso.best_params_)\n",
    "print(\"Best Lasso Score:\", grid_search_lasso.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f609e6f3",
   "metadata": {},
   "source": [
    "### Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd1e84",
   "metadata": {},
   "source": [
    "SVM do not need data preprocessing with StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91109527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Train):\n",
      "[[ 22   9]\n",
      " [  8 208]]\n",
      "\n",
      "Classification Report (Train):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72        31\n",
      "           1       0.96      0.96      0.96       216\n",
      "\n",
      "    accuracy                           0.93       247\n",
      "   macro avg       0.85      0.84      0.84       247\n",
      "weighted avg       0.93      0.93      0.93       247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define and train the model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix (Train):\")\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print(\"\\nClassification Report (Train):\")\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014308c8",
   "metadata": {},
   "source": [
    "### Ensemble model (e.g. Random Forest or Gradient Boosting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6090db8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                           cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e800f78",
   "metadata": {},
   "source": [
    "### Neural network implemented in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ad47478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 10/50, Loss: 0.4525\n",
      "Epoch 20/50, Loss: 0.2368\n",
      "Epoch 30/50, Loss: 0.1801\n",
      "Epoch 40/50, Loss: 0.1557\n",
      "Epoch 50/50, Loss: 0.1327\n",
      "Training Complete.\n",
      "\n",
      "Neural Network Training Accuracy: 0.9393\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare Data for PyTorch\n",
    "# Convert arrays to FloatTensors\n",
    "X_tensor = torch.FloatTensor(X_train)\n",
    "y_tensor = torch.FloatTensor(y_train.values).view(-1, 1) # Reshape to column vector\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 2. Define the Neural Network\n",
    "class LungCancerNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LungCancerNet, self).__init__()\n",
    "        # Simple architecture: Input -> Hidden (16 neurons) -> Output (1 neuron)\n",
    "        self.layer1 = nn.Linear(input_dim, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(16, 1) \n",
    "        # We will use BCEWithLogitsLoss, which includes Sigmoid, so no final activation here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Model\n",
    "input_dim = X_train.shape[1]\n",
    "model = LungCancerNet(input_dim)\n",
    "\n",
    "# 3. Define Loss and Optimizer\n",
    "# pos_weight handles class imbalance (Ratio of Negatives / Positives)\n",
    "# Based on your confusion matrix (~37 Neg / ~210 Pos), imbalance is actually low quantity of negatives.\n",
    "# Actually, since you want to catch Cancer (1), usually we weight the positive class. \n",
    "# But here Cancer(1) is the MAJORITY. We need to weight the MINORITY (0) higher if we want to catch it.\n",
    "# For simplicity, let's use standard loss first, but you can add pos_weight if needed.\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. Training Loop\n",
    "epochs = 50\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}')\n",
    "\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# 5. Evaluate on Training Data (Quick Check)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(X_tensor)\n",
    "    y_pred_probs = torch.sigmoid(y_pred_logits)\n",
    "    y_pred_cls = (y_pred_probs > 0.5).float()\n",
    "    \n",
    "    accuracy = (y_pred_cls.eq(y_tensor).sum() / float(y_tensor.shape[0])).item()\n",
    "    print(f\"\\nNeural Network Training Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
